{
  "paragraphs": [
    {
      "text": "// Implement in Spark the sliding window aggregation algorithm from the lecture.  For each record compute the average ride distance and the average passenger occupancy during the last hour.\n// Make sure that sorting criteria, aggregation function and window length can be easily changed. Make sure that you algorithm is minimal and follows the one from the lecture. \n// Use RDD API in Scala or Python or Java. Send your solution to my email as a zeppelin notebook.",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:02+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1648284090277_1837970131",
      "id": "paragraph_1648284090277_1837970131",
      "dateCreated": "2022-03-26T09:41:30+0100",
      "dateStarted": "2022-04-09T17:35:02+0200",
      "dateFinished": "2022-04-09T17:35:23+0200",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:46629",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "import org.apache.spark.rdd.RDD\nimport scala.reflect.ClassTag",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:23+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1648289365331_518689489",
      "id": "paragraph_1648289365331_518689489",
      "dateCreated": "2022-03-26T11:09:25+0100",
      "dateStarted": "2022-04-09T17:35:23+0200",
      "dateFinished": "2022-04-09T17:35:23+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46630",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.rdd.RDD\nimport scala.reflect.ClassTag\n"
          }
        ]
      }
    },
    {
      "text": "trait Settings {\n    type RDD[T] = org.apache.spark.rdd.RDD[T]\n    type DataFrame = org.apache.spark.sql.DataFrame\n    type MachineId = Int\n    type Rank = Int\n    \n    val numberOfMachines = 10\n    val firstMachineId = 0\n    val lastMachineId = numberOfMachines-1\n}",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:23+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1648300295059_1393060937",
      "id": "paragraph_1648300295059_1393060937",
      "dateCreated": "2022-03-26T14:11:35+0100",
      "dateStarted": "2022-04-09T17:35:23+0200",
      "dateFinished": "2022-04-09T17:35:23+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46631",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined trait Settings\n"
          }
        ]
      }
    },
    {
      "text": "object Conversions {\n    implicit val int2OptionInt: Int => Option[Int] = n => Some(n)\n    implicit val long2OptionInt: Long => Option[Int] = n => Some(n.toInt)\n}",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:23+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649497205620_2137448632",
      "id": "paragraph_1649497205620_2137448632",
      "dateCreated": "2022-04-09T11:40:05+0200",
      "dateStarted": "2022-04-09T17:35:23+0200",
      "dateFinished": "2022-04-09T17:35:24+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46632",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined object Conversions\n"
          }
        ]
      }
    },
    {
      "text": "trait MapReduce extends java.io.Serializable {\n    this: Settings =>\n    import org.apache.spark.Partitioner\n    \n    // force user to use partitioner for appriopriate type of key with static typing\n    abstract class genericPartitioner[K] extends Partitioner { \n        override def getPartition(key: Any) = convert(key)\n        override def numPartitions = numberOfMachines\n        \n        def convert(key: Any): Int\n    }\n    \n    \n    implicit object minimalAlgorithmsPartitioner extends genericPartitioner[Int] {\n        def convert(key: Any): Int = { \n            val machineId = key.asInstanceOf[Int]\n            assert(0 <= machineId && machineId <= lastMachineId, s\"bad machineId $machineId\")\n            machineId\n        }\n    }\n    \n    \n    def mapReduce[K : ClassTag, T1, T2 : ClassTag, R : ClassTag](dataset: RDD[T1], \n                                mapFun: (T1) => (K, T2), \n                                reduce: ((K, Iterable[T2])) => R)\n                                (implicit partitioner: genericPartitioner[K]): RDD[R] = {\n                                    \n        return dataset.map(mapFun).groupByKey(partitioner).map(reduce)\n    }\n    \n    \n    def mapReduce[K : ClassTag, T1, T2 : ClassTag, R : ClassTag](dataset: RDD[T1], \n                                mapFun: T1 => List[(K, T2)], \n                                reduce: ((K, Iterable[T2])) => R)\n                                (implicit partitioner: genericPartitioner[K], \n                                d: DummyImplicit): RDD[R] = {\n                                    \n        return dataset.flatMap(mapFun).groupByKey(partitioner).map(reduce)\n    }\n    \n    \n    def mapReduce[K : ClassTag, T1, T2 : ClassTag, R : ClassTag](dataset: RDD[T1], \n                                mapFun: T1 => (K, T2), \n                                reduce: ((K, Iterable[T2])) => List[R])\n                                (implicit partitioner: genericPartitioner[K], \n                                d1: DummyImplicit,\n                                d2: DummyImplicit): RDD[R] = {\n        \n        return dataset.map(mapFun).groupByKey(partitioner).flatMap(reduce)\n    }\n                                    \n                                    \n    def mapReduce[K : ClassTag, T1, T2 : ClassTag, R : ClassTag](dataset: RDD[T1], \n                                mapFun: T1 => List[(K, T2)], \n                                reduce: ((K, Iterable[T2])) => List[R])\n                                (implicit partitioner: genericPartitioner[K], \n                                d1: DummyImplicit,\n                                d2: DummyImplicit,\n                                d3: DummyImplicit): RDD[R] = {\n                                    \n        return dataset.flatMap(mapFun).groupByKey(partitioner).flatMap(reduce)\n    }\n}",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:24+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1648298583200_1614264328",
      "id": "paragraph_1648298583200_1614264328",
      "dateCreated": "2022-03-26T13:43:03+0100",
      "dateStarted": "2022-04-09T17:35:24+0200",
      "dateFinished": "2022-04-09T17:35:25+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46633",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined trait MapReduce\n"
          }
        ]
      }
    },
    {
      "text": "object MinimalALgorithms extends MapReduce with Settings with java.io.Serializable {\n\n    import org.apache.spark.Partitioner\n    \n    def getMachineId(): MachineId = {\n        import org.apache.spark.TaskContext\n        TaskContext.getPartitionId()\n    }\n    \n    implicit val int2OptionInt: Int => Option[Int] = n => Some(n)\n    implicit val long2OptionInt: Long => Option[Int] = n => Some(n.toInt)\n\n    \n    def terasort[E : ClassTag, T : ClassTag](dataset: RDD[E], \n                       pickToSort: E => T,\n                       rddSize: Option[Int] = None)\n                       (implicit ev: T => Ordered[T]): RDD[E] = {\n        \n        import scala.math.log\n        assert(numberOfMachines > 0, \"number of machines must be non-zero!\")\n        \n        val n = if (rddSize.isDefined) rddSize.get else dataset.count()\n        if (n == 0) return dataset \n        \n        val t = numberOfMachines\n        val m = (n.toDouble / t).ceil\n        \n        def find_q_prob = {\n            val q_prob: Double = (log(n*t).toDouble) / m\n            q_prob\n        }\n        \n        val q_prob = find_q_prob               \n        val rnd = new scala.util.Random\n        \n        def probabilityMet(q: Double, rnd: scala.util.Random) = rnd.nextDouble() <= q\n\n        sealed trait ShuffleData extends java.io.Serializable\n        final case class Sample(val sample: List[T]) extends ShuffleData\n        final case class MachineData(val data: List[E]) extends ShuffleData\n        \n        sealed case class Boundaries(val boundaries: Array[T]) extends java.io.Serializable\n\n\n        def round1Map(arr: Array[E]): List[(MachineId, ShuffleData)] = {\n            val sample = arr.filter(_ => probabilityMet(q_prob, rnd)).map(pickToSort)\n            val sampleResult = for (i <- 0 to lastMachineId) yield (i, new Sample(sample.toList))\n            val currMachineId = getMachineId()\n            val mapResult = (currMachineId, new MachineData(arr.toList))::sampleResult.toList\n            mapResult\n        }\n        \n\n        def round1Reduce(keyIterable: (MachineId, Iterable[ShuffleData])): (Boundaries, MachineData) = {\n            val (key, dataIterable) = keyIterable\n            \n            val (samples, machineDataArr) = dataIterable.partition(_.isInstanceOf[Sample])\n            val MachineData(machineData) = machineDataArr.toList.head\n            val samplesSorted = samples.flatMap(_ match { case Sample(sample) => sample; case _ => throw new Exception(\"wrong data in sample!\")}).toArray.sorted\n            val s = samplesSorted.size\n            val step = (s.toDouble / t).ceil.toInt\n            val boundaries = for (i <- 1 to t - 1) yield samplesSorted((i*step) min (s-1))\n            \n            (new Boundaries(boundaries.toArray), new MachineData(machineData))\n        }\n        \n        \n        def round2Map(boundariesData: (Boundaries, MachineData)): List[(MachineId, List[E])] = {\n            val (Boundaries(sortedBoundaries), machineData) = boundariesData\n            val boundaryElemToPartition = (0 to t-2).zip(sortedBoundaries).toMap\n            val sortedMachineData = machineData.data.sortBy(pickToSort)\n            val lastBoundIndex = sortedBoundaries.size-1\n            \n            \n            def getNextBoundIdx(elem: E, i: Int): Int = {\n                import scala.util.Try\n                val next_i = Try(sortedBoundaries.zip(0 to t-2).filter{ case (b, i) => pickToSort(elem) <= b }.head._2).toOption.getOrElse(t-1)\n                if (!(next_i > i)) throw new Exception(\"zepsuty index granpride\")\n                next_i\n            }\n            \n            val add2map = (map: Map[Int, List[E]], elem: E, i: Int) => map + (i -> (elem::map(i)))\n            val (_, splitedElemsByPartition) = sortedMachineData.\n                                               foldLeft((0, Map[Int, List[E]]().withDefaultValue(List[E]())))\n                                               { (acc, elem) => val (i, map) = (acc._1, acc._2)\n                                                  if (i == lastBoundIndex+1) (i, add2map(map, elem, i))\n                                                  else if (pickToSort(elem) <= sortedBoundaries(i)) (i, add2map(map, elem, i))\n                                                  else {val next_i = getNextBoundIdx(elem, i); (next_i, add2map(map, elem, next_i)) }\n                                               }\n            \n            for (i <- (0 to t-1).toList) yield (i, splitedElemsByPartition(i))\n        }\n        \n        \n        def round2Reduce(keyIterable: (MachineId, Iterable[List[E]])): List[E] = {\n            val (key, dataIterable) = keyIterable\n            dataIterable.flatMap(identity).toArray.sortBy(pickToSort).toList\n        }\n\n        \n        val round1result = mapReduce(dataset.repartition(numberOfMachines).glom(), round1Map(_), round1Reduce(_))\n        val round2result = mapReduce(round1result, round2Map(_), round2Reduce(_))\n        val teraSortResult = round2result\n        \n        teraSortResult\n    }\n    \n    \n    def prefixSum[E : ClassTag, T : ClassTag, W](dataset: RDD[E],\n                          pickToSort: E => T, \n                          weight: E => W, \n                          sum: (W, W) => W,\n                          rddSize: Option[Int] = None)\n                          (implicit ev: T => Ordered[T]): RDD[(E, W)] = {\n                              \n        assert(numberOfMachines > 0, \"number of machines must be non-zero!\")\n        \n        sealed trait ShuffleData extends java.io.Serializable\n        final case class MachineSum(val sum: W) extends ShuffleData\n        final case class MachineData(val data: List[E]) extends ShuffleData\n        \n        val n = if (rddSize.isDefined) rddSize.get else dataset.count()\n        if (n == 0) return dataset.map(elem => (elem, weight(elem)))\n        \n        val t = numberOfMachines\n        val m = (n.toDouble / t).ceil\n        \n        \n        def roundMap(arr: Array[E]): List[(MachineId, ShuffleData)] = {\n            import scala.util.Try\n            val machineSum: Option[W] = Try(arr.map(elem => weight(elem)).reduce(sum)).toOption\n            val currMachineId = getMachineId()\n            lazy val machineSumShuffle = (for (i <- (currMachineId + 1 to t-1).toList) yield (i, MachineSum(machineSum.get)))\n            val machineDataShuffle = (currMachineId, new MachineData(arr.toList))\n            val mapResult = if (machineSum.isDefined) machineDataShuffle::machineSumShuffle else List(machineDataShuffle)\n            mapResult\n        }\n        \n        def roundReduce(keyIterable: (MachineId, Iterable[ShuffleData])): List[(E, W)] = {\n            val (key, dataIterable) = keyIterable\n            \n            val (previousMachineSums, machineDataArr) = dataIterable.partition(_.isInstanceOf[MachineSum])\n            val MachineData(machineData) = machineDataArr.toList.head\n            \n            lazy val totalSumPreviousMachines: W = (previousMachineSums: Iterable[ShuffleData]).map{ _ match { case MachineSum(sum) => sum\n                                                                              case _ => throw new Exception(\"dane maszyny wsrod sum czesciowych\")\n                                                                            } }.reduce(sum): W\n\n                                                      \n            lazy val startWeight = weight(machineData.head)\n            lazy val startAcc = (startWeight, List[W](startWeight))\n            \n            lazy val foldStep = (acc: (W, List[W]), elem: E) => {\n                                val (prefixSum, l) = acc\n                                val newPrefixSum: W = sum(prefixSum, weight(elem)): W\n                                (newPrefixSum, newPrefixSum::l)\n                            }\n                                \n            lazy val (_, prefixSumReversed) = machineData.tail.foldLeft(startAcc)(foldStep)\n            lazy val prefixSum = prefixSumReversed.reverse\n            \n            val currMachineId = getMachineId()\n            lazy val globalPrefixSum = if (currMachineId == firstMachineId) prefixSum \n                                  else prefixSum.map(sum(totalSumPreviousMachines, _))\n                                  \n            lazy val reduceResult = machineData.zip(globalPrefixSum)\n\n            if (machineData.nonEmpty) reduceResult else Nil\n        }\n        \n        val rddSorted = terasort[E, T](dataset, pickToSort, rddSize)\n        val prefixSumResult = mapReduce(rddSorted.glom(), roundMap(_), roundReduce(_))\n\n        prefixSumResult\n    }\n    \n    \n    def ranking[E : ClassTag, T : ClassTag](dataset: RDD[E], \n                      pickToSort: E => T,\n                      rddSize: Option[Int] = None)\n                      (implicit ev: T => Ordered[T]): RDD[(E, Rank)] = {\n                                           \n        prefixSum[E, T, Rank](dataset, pickToSort, _ => 1, _ + _, rddSize)\n    }\n    \n    \n    def perfectBalanceSort[E : ClassTag, T : ClassTag](dataset: RDD[E],\n                                 pickToSort: E => T,\n                                 rddSize: Option[Int] = None)\n                                 (implicit ev: T => Ordered[T]): RDD[(E, Rank)] = {\n        \n        assert(numberOfMachines > 0, \"number of machines must be non-zero!\")\n\n        val n = if (rddSize.isDefined) rddSize.get else dataset.count()\n        if (n == 0) return dataset.map( e => (e, 0))\n        \n        val t = numberOfMachines\n        val m = (n.toDouble / t).ceil.toInt\n        \n        def roundMap(rankingPair: (E, Rank)): (MachineId, (E, Rank)) = {\n            val (elem, rank) = rankingPair\n            val receiverPartitionId = (rank-1) / m\n            (receiverPartitionId, (elem, rank))\n        }\n        \n        def roundReduce(keyIterable: (MachineId, Iterable[(E, Rank)])): List[(E, Rank)] = {\n            keyIterable._2.toList.sortBy{ case (elem, rank) => rank }\n        }\n        \n        val rddRanking = ranking[E, T](dataset, pickToSort, rddSize)\n        val prefixSumResult = mapReduce(rddRanking, roundMap(_), roundReduce(_))\n        \n        prefixSumResult\n    }\n    \n    \n    def slidingAggregation[E : ClassTag, T : ClassTag, W : ClassTag](dataset: RDD[E], \n                                    pickToSort: E => T, \n                                    windowSize: Int,\n                                    weight: E => W,\n                                    aggregate: List[W] => W,\n                                    rddSize: Option[Int] = None)\n                                    (implicit ev: T => Ordered[T]): RDD[(E, W)] = {\n                                        \n        val slidingAggregationManyRes = slidingAggregationMany[E, T, W](dataset, pickToSort, windowSize, List(weight), List(aggregate), rddSize)\n        val result = slidingAggregationManyRes.map{ case (elem, arr_W) => (elem, arr_W.head)}\n        \n        result\n    }\n    \n    \n    def slidingAggregationMany[E : ClassTag, T : ClassTag, W : ClassTag](dataset: RDD[E], \n                                    pickToSort: E => T, \n                                    windowSize: Int,\n                                    weights: List[E => W],\n                                    aggregates: List[List[W] => W],\n                                    rddSize: Option[Int] = None)\n                                    (implicit ev: T => Ordered[T]): RDD[(E, Array[W])] = {\n        \n        assert(numberOfMachines > 0, \"number of machines must be non-zero!\")\n        assert(weights.size == aggregates.size, \"different number of weights and aggregates functions\")\n        \n        val n = if (rddSize.isDefined) rddSize.get else dataset.count()\n        if (n == 0) return dataset.map(e => (e, Array[W]()))\n        \n        val t = numberOfMachines\n        val m = (n.toDouble / t).ceil.toInt\n        \n                                        \n        sealed trait ShuffleData extends java.io.Serializable\n        final case class MachineAggregation(val aggregatedFrom: Int, val partialResult: Array[W]) extends ShuffleData\n        final case class RelevantToMachine(val dataWithRank: Set[(E, Int)]) extends ShuffleData\n        final case class MachineData(val data: Array[(E, Int)]) extends ShuffleData\n                                        \n        \n        def roundMap(arr: Array[(E, Int)]): List[(MachineId, ShuffleData)] = {\n            lazy val currMachineId: Int = getMachineId()\n            lazy val emptyShuffle = List[(MachineId, ShuffleData)]()\n            \n            lazy val dataWeights: List[Array[W]] = arr.map{ case (elem, rank) => (for (weight: (E => W) <- weights) yield weight(elem)).toArray }.toList\n            lazy val machineAggregationRes = for ((aggregate, i) <- aggregates zip (0 to weights.size-1)) yield aggregate(dataWeights.map(_.apply(i)).toList)\n            lazy val machineAggregated = MachineAggregation(currMachineId, machineAggregationRes.toArray)\n            \n            lazy val singleRelevantMachineId = currMachineId + 1\n            lazy val relevantMachineId: Int = currMachineId + ((windowSize-1).toDouble / m).floor.toInt\n            lazy val relevantMachineIdexes: Set[Int] = Set(relevantMachineId, (relevantMachineId+1) min (lastMachineId))\n\n\n            lazy val localAggregationShuffle: Set[(Int, ShuffleData)] = (for (i <- 0 to lastMachineId) yield (i, machineAggregated)).toSet\n            lazy val machineDataShuffle: Set[(Int, ShuffleData)] = Set((currMachineId, MachineData(arr)))\n            lazy val relevantMachineShuffle: Set[(Int, ShuffleData)] = if (windowSize <= m && currMachineId < lastMachineId) Set((singleRelevantMachineId, RelevantToMachine(arr.toSet)))\n                                                                  else if (windowSize > m && relevantMachineId <= lastMachineId) relevantMachineIdexes.map(i => (i, RelevantToMachine(arr.toSet)))\n                                                                  else Set()\n                \n            lazy val shuffleData = machineDataShuffle union localAggregationShuffle.toSet union relevantMachineShuffle\n            \n            if (arr.nonEmpty) shuffleData.toList else emptyShuffle\n        }\n        \n        \n        def roundReduce(keyIterable: (MachineId, Iterable[ShuffleData])): List[(E, Array[W])] = {\n            val (key, dataIterable) = keyIterable\n            val currMachineId: Int = getMachineId()\n            val emptyRes = List[(E, Array[W])]()\n            \n            import scala.util.Try\n\n            val (machineAggregations, machineDataAndRelevantToMachine) = dataIterable.partition(_.isInstanceOf[MachineAggregation])\n            val (relevantToMachine, machineDataArr) = machineDataAndRelevantToMachine.partition(_.isInstanceOf[RelevantToMachine])\n            \n            if (machineDataArr.isEmpty) return emptyRes\n            \n            \n            val MachineData(machineData) = machineDataArr.head\n            val dataWeights: Array[Array[W]] = machineData.map{ case (elem, rank) => (for (weight <- weights) yield weight(elem)).toArray }  \n\n            val machineIdPartialAggregation: Map[MachineId, Array[W]] = machineAggregations.map{ _ match { case MachineAggregation(aggregatedFrom, partialResult) => (aggregatedFrom, partialResult); \n                                                                                                             case _ => throw new Exception(\"not MachineAggregation object!\") }\n                                                                                                 }.toMap\n            \n                                                                                      \n            val relevant = relevantToMachine.map{ _ match { case RelevantToMachine(dataWithRank) => dataWithRank\n                                                            case _ => throw new Exception(\"not RelevantToMachine object!\") } }\n                                                .reduceOption(_ union _)\n                                                .getOrElse(Set())\n                                                .toArray\n                                                .sortBy{ case (_, rank) => rank }\n                                                .map(_._1)\n            \n            \n            def calculateWindow(elemRank: Rank): Array[W] = {\n                def windowStart(rank: Rank) = (rank - windowSize + 1)\n                def rankToMachine(rank: Rank): Int = (windowStart(rank) / m.toDouble).ceil.toInt -1\n                implicit def bool2int(b:Boolean) = if (b) 1 else 0\n                \n                val α: Int = rankToMachine(elemRank)\n                lazy val leftMostRelMachine = rankToMachine(machineData.head._2)\n                lazy val lastRelevantIdx = (relevant.size == 2) + (leftMostRelMachine < α && leftMostRelMachine >= 0)*m + (rankToMachine(elemRank) >= 0)*m\n\n                def takeRelevant(elemRank: Rank): Array[Array[W]] = {\n                    lazy val windowStartRank = (windowStart(elemRank)) max 1\n                    lazy val firstRelevantMachineId = leftMostRelMachine\n                    lazy val startRelevantIdx = (relevant.size == 2) + (leftMostRelMachine < α && firstRelevantMachineId >= 0)*m + ((windowStartRank-1) % m)\n                    lazy val rel = relevant.slice(startRelevantIdx, lastRelevantIdx)\n                    lazy val res = rel.map(elem => (for (weight <- weights) yield weight(elem)).toArray)\n                    if (α < currMachineId) res else Array()\n                }\n                \n                def takePartialAggregations(elemRank: Rank): Array[Array[W]] = {\n                    (for (j <- ((α+1) max 0 to (currMachineId-1)).toList) yield machineIdPartialAggregation(j)).toArray\n                }\n                \n                def takeOnThisMachine(elemRank: Rank): Array[Array[W]] = {\n                    def rankToIndexesRange(rank: Rank): (Int, Int) = { \n                        val lastElemIdx = (rank-1) % m \n                        ((lastElemIdx - windowSize + 1), lastElemIdx) \n                    }\n                    val (first, last) = rankToIndexesRange(elemRank)\n                    dataWeights.slice(first, last+1)\n                }\n                \n                lazy val w1 = takeRelevant(elemRank)\n                lazy val w2 = takePartialAggregations(elemRank)\n                lazy val w3 = takeOnThisMachine(elemRank)\n                lazy val w1_w2_w3 = w1 ++ w2 ++ w3        \n\n\n                lazy val windowRes = for ((aggregate, i) <- aggregates zip (0 to weights.size-1)) yield aggregate(w1_w2_w3.map(_.apply(i)).toList)\n                \n                windowRes.toArray\n            }\n\n            lazy val slidingAggregationRes = machineData.map{ case (elem, rank) => (elem, calculateWindow(rank)) }.toList\n            slidingAggregationRes\n        }\n        \n        val datasetPerfectSorted = perfectBalanceSort[E, T](dataset, pickToSort)\n        val slidingAggregationResult = mapReduce(datasetPerfectSorted.glom(), roundMap(_), roundReduce(_))\n        \n        slidingAggregationResult\n    }\n}",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:25+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1648284182180_727641940",
      "id": "paragraph_1648284182180_727641940",
      "dateCreated": "2022-03-26T09:43:02+0100",
      "dateStarted": "2022-04-09T17:35:25+0200",
      "dateFinished": "2022-04-09T17:35:28+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46634",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[33mwarning: \u001b[0mthere was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\ndefined object MinimalALgorithms\n"
          }
        ]
      }
    },
    {
      "text": "//not very beatiful way to convert data but its not important in this task\nimport org.apache.spark.sql.functions._ \nval filePath = \"/home/zadanie1/yellow_tripdata.csv\"\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(filePath)\n\nval zadanieDataset = df.select(col(\"passenger_count\").cast(\"double\"), col(\"trip_distance\").cast(\"double\"))\n                .withColumn(\"id\",monotonicallyIncreasingId)\n                .rdd.map(row => (row(2).asInstanceOf[Long].toInt, row(1).asInstanceOf[Double], row(0).asInstanceOf[Double].toInt))\n                \n// val smallerSize = 1000000\n// val smallerZadadanieDataset = sc.parallelize(zadanieDataset.take(smallerSize).toList)",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:28+0200",
      "progress": 66,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=0",
              "$$hashKey": "object:48033"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649493556794_553492453",
      "id": "paragraph_1649493556794_553492453",
      "dateCreated": "2022-04-09T10:39:16+0200",
      "dateStarted": "2022-04-09T17:35:28+0200",
      "dateFinished": "2022-04-09T17:35:35+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46635",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[33mwarning: \u001b[0mthere was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\nimport org.apache.spark.sql.functions._\n\u001b[1m\u001b[34mfilePath\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /home/zadanie1/yellow_tripdata.csv\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [VendorID: string, tpep_pickup_datetime: string ... 16 more fields]\n\u001b[1m\u001b[34mzadanieDataset\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Double, Int)]\u001b[0m = MapPartitionsRDD[16] at map at <console>:32\n"
          }
        ]
      }
    },
    {
      "text": "def pickToSort(elem: (Int, Double, Int)): Int =  elem._1\ndef takeRideDistance(elem: (Int, Double, Int)): Any = elem._2\ndef takePassengerCount(elem: (Int, Double, Int)): Any = elem._3\n\ndef aggregatePassengerCount(l: List[Any]): Int = l.asInstanceOf[List[Int]].reduce(_ + _)\ndef aggregateRideDistance(l: List[Any]): Double = l.asInstanceOf[List[Double]].reduce(_ + _)\n    \nval windowSize = 1000",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:35+0200",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649506287055_1404382185",
      "id": "paragraph_1649506287055_1404382185",
      "dateCreated": "2022-04-09T14:11:27+0200",
      "dateStarted": "2022-04-09T17:35:35+0200",
      "dateFinished": "2022-04-09T17:35:35+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46636",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mpickToSort\u001b[0m: \u001b[1m\u001b[32m(elem: (Int, Double, Int))Int\u001b[0m\n\u001b[1m\u001b[34mtakeRideDistance\u001b[0m: \u001b[1m\u001b[32m(elem: (Int, Double, Int))Any\u001b[0m\n\u001b[1m\u001b[34mtakePassengerCount\u001b[0m: \u001b[1m\u001b[32m(elem: (Int, Double, Int))Any\u001b[0m\n\u001b[1m\u001b[34maggregatePassengerCount\u001b[0m: \u001b[1m\u001b[32m(l: List[Any])Int\u001b[0m\n\u001b[1m\u001b[34maggregateRideDistance\u001b[0m: \u001b[1m\u001b[32m(l: List[Any])Double\u001b[0m\n\u001b[1m\u001b[34mwindowSize\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 1000\n"
          }
        ]
      }
    },
    {
      "text": "object Zadanie extends java.io.Serializable {\n    \n    type YellowTaxiRDD = RDD[(Int, Double, Int)]\n    type DistanceWindowRDD = RDD[Double]\n    type PassengerOccupancyRDD = RDD[Int]\n    \n    type Distance = Double\n    type Occupancy = Int\n    \n    def policzSrednieAlgorytmemMinimalnym(dataset: RDD[(Int, Double, Int)]): (DistanceWindowRDD, PassengerOccupancyRDD) = {\n        val slidingAggregationRes = MinimalALgorithms.slidingAggregationMany[(Int, Double, Int), Int, Any](dataset, \n                                                                                                          pickToSort, \n                                                                                                          windowSize, \n                                                                                                          List(takeRideDistance, takePassengerCount), \n                                                                                                          List(aggregateRideDistance, aggregatePassengerCount))\n                                                                                                           \n        val averagesWindow = slidingAggregationRes.map{ case (inputElem, aggArr) => (aggArr(0), aggArr(1)) }\n        val distanceWindow = averagesWindow.map{ case (distance, _) => distance.asInstanceOf[Double] }\n        val passegerOccupancyWindow = averagesWindow.map{ case (_, occupancy) => occupancy.asInstanceOf[Int] }\n        \n        (distanceWindow, passegerOccupancyWindow)\n    }\n    \n    \n    \n    def slidingSingleTest[E, T, W](dataset: Array[E],\n                                  pickToSort: E => T,\n                                  windowSize: Int,\n                                  weight: E => W,\n                                  aggregate: List[W] => W)\n                                  (implicit ev: T => Ordered[T], \n                                    classTag_W: ClassTag[W],\n                                    classTag_T: ClassTag[T], \n                                    classTag_E: ClassTag[E]): List[(E, W)] = {\n        val sortedDataset = dataset.sortBy(pickToSort).toList\n        val windowRes = for (i <- 0 to sortedDataset.size-1) yield aggregate(sortedDataset.slice(i-windowSize+1, i+1).map(weight).toList) //i+1\n        sortedDataset zip windowRes                               \n    }\n    \n    \n    def slidingManyTest[E, T, W](dataset: Array[E],\n                                  pickToSort: E => T,\n                                  windowSize: Int,\n                                  weights: List[E => W],\n                                  aggregates: List[List[W] => W])\n                                  (implicit ev: T => Ordered[T], \n                                    classTag_W: ClassTag[W],\n                                    classTag_T: ClassTag[T], \n                                    classTag_E: ClassTag[E]): List[(E, Array[W])] = {\n        val sortedDataset = dataset.sortBy(pickToSort)\n        val windowWeights = for (i <- 0 to sortedDataset.size-1) yield sortedDataset.slice(i-windowSize+1, i+1).map(elem => (for (weight <- weights) yield weight(elem))) //i+1\n        val windowsRes = for (window <- windowWeights) yield (for ((aggregate, i) <- aggregates zip (0 to aggregates.size-1)) yield aggregate(window.map(_.apply(i)).toList)).toArray\n        sortedDataset.toList zip windowsRes                        \n    }\n\n    \n    def singleTest[E : ClassTag, T: ClassTag, W : ClassTag](dataset: RDD[E], \n                                                 pickToSort: E => T, \n                                                 windowSize: Int, \n                                                 weights: List[E => W], \n                                                 aggregates: List[List[W] => W])\n                                                 (implicit ev: T => Ordered[T]): Unit = {\n                                                     \n        val minimalRes = MinimalALgorithms.slidingAggregationMany[E, T, W](dataset, \n                                                                          pickToSort, \n                                                                          windowSize, \n                                                                          weights, \n                                                                          aggregates).collect().toList.map(_._2.toList)\n        \n        val testRes = slidingManyTest[E, T, W](dataset.collect().toArray, \n                                                                         pickToSort, \n                                                                         windowSize, \n                                                                         weights, \n                                                                         aggregates).map(e => (e._1, e._2.toList)).map(_._2)\n                                                                         \n        assert(minimalRes == testRes, s\"WRONG, $windowSize\")\n    }\n    \n    \n    def test[E : ClassTag, T : ClassTag, W : ClassTag](dataset: RDD[E], \n                                                      pickToSort: E => T, \n                                                      windowSizes: List[Int], \n                                                      weights: List[E => W], \n                                                      aggregates: List[List[W] => W])\n                                                      (implicit ev: T => Ordered[T]): Unit = {\n                                                           \n        val testDatasetSize = 50000\n        val testDataset = sc.parallelize(dataset.take(testDatasetSize).toList)\n                                                           \n        windowSizes.foreach(windowSize => singleTest[E, T, W](testDataset, pickToSort, windowSize, weights, aggregates))\n        println(\"CORRECT!\")\n    }\n}",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:35+0200",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649508397235_1961581423",
      "id": "paragraph_1649508397235_1961581423",
      "dateCreated": "2022-04-09T14:46:37+0200",
      "dateStarted": "2022-04-09T17:35:35+0200",
      "dateFinished": "2022-04-09T17:35:36+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46637",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined object Zadanie\n"
          }
        ]
      }
    },
    {
      "text": "import Conversions._\n\nZadanie.test[(Int, Double, Int), Int, Any](zadanieDataset, \n             pickToSort(_), \n             List(10, 100, 1000),\n             List(takeRideDistance, takePassengerCount), \n             List(aggregateRideDistance, aggregatePassengerCount))",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:35:36+0200",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=1",
              "$$hashKey": "object:49174"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=2",
              "$$hashKey": "object:49175"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=3",
              "$$hashKey": "object:49176"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=4",
              "$$hashKey": "object:49177"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=5",
              "$$hashKey": "object:49178"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=6",
              "$$hashKey": "object:49179"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=7",
              "$$hashKey": "object:49180"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=8",
              "$$hashKey": "object:49181"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=9",
              "$$hashKey": "object:49182"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=10",
              "$$hashKey": "object:49183"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=11",
              "$$hashKey": "object:49184"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=12",
              "$$hashKey": "object:49185"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=13",
              "$$hashKey": "object:49186"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=14",
              "$$hashKey": "object:49187"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=15",
              "$$hashKey": "object:49188"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=16",
              "$$hashKey": "object:49189"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=17",
              "$$hashKey": "object:49190"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=18",
              "$$hashKey": "object:49191"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=19",
              "$$hashKey": "object:49192"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649507321184_2055264697",
      "id": "paragraph_1649507321184_2055264697",
      "dateCreated": "2022-04-09T14:28:41+0200",
      "dateStarted": "2022-04-09T17:35:36+0200",
      "dateFinished": "2022-04-09T17:36:28+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46638",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "CORRECT!\nimport Conversions._\n"
          }
        ]
      }
    },
    {
      "text": "val (distanceSumWindows, occupancySumWindows) = Zadanie.policzSrednieAlgorytmemMinimalnym(zadanieDataset)\nval distanceAverageWindows = distanceSumWindows.map( sum => (math floor (sum / windowSize) * 100) / 100)\nval occupancyAverageWindows = occupancySumWindows.map( sum => (math floor (sum.toDouble / windowSize) * 100) / 100)",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:36:28+0200",
      "progress": 25,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=20",
              "$$hashKey": "object:49370"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=21",
              "$$hashKey": "object:49371"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=22",
              "$$hashKey": "object:49372"
            },
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=23",
              "$$hashKey": "object:49373"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649507619584_2119467706",
      "id": "paragraph_1649507619584_2119467706",
      "dateCreated": "2022-04-09T14:33:39+0200",
      "dateStarted": "2022-04-09T17:36:28+0200",
      "dateFinished": "2022-04-09T17:36:36+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46639",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mdistanceSumWindows\u001b[0m: \u001b[1m\u001b[32mZadanie.DistanceWindowRDD\u001b[0m = MapPartitionsRDD[107] at map at <console>:56\n\u001b[1m\u001b[34moccupancySumWindows\u001b[0m: \u001b[1m\u001b[32mZadanie.PassengerOccupancyRDD\u001b[0m = MapPartitionsRDD[108] at map at <console>:57\n\u001b[1m\u001b[34mdistanceAverageWindows\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Double]\u001b[0m = MapPartitionsRDD[109] at map at <console>:40\n\u001b[1m\u001b[34moccupancyAverageWindows\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Double]\u001b[0m = MapPartitionsRDD[110] at map at <console>:41\n"
          }
        ]
      }
    },
    {
      "text": "//dla uproszczenia dziele zawsze przez rozmiar 1000 nawet jesli okno jest krotsze bo tych okien na ponad milion istniejacych jest tylko 999 i jest to szczegol nie zwiazany z implementacja minimalnych algorytmow\nprintln(distanceAverageWindows.take(2000).toList.slice(1000, 2000))",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:36:36+0200",
      "progress": 98,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=24",
              "$$hashKey": "object:49423"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649508631582_718418568",
      "id": "paragraph_1649508631582_718418568",
      "dateCreated": "2022-04-09T14:50:31+0200",
      "dateStarted": "2022-04-09T17:36:36+0200",
      "dateFinished": "2022-04-09T17:37:19+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46640",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "List(2.52, 2.52, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.52, 2.53, 2.53, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.51, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.53, 2.54, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5, 2.5, 2.51, 2.5, 2.51, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.51, 2.51, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.51, 2.51, 2.5, 2.5, 2.5, 2.5, 2.5, 2.49, 2.49, 2.49, 2.49, 2.49, 2.48, 2.49, 2.49, 2.49, 2.48, 2.48, 2.49, 2.49, 2.49, 2.49, 2.49, 2.49, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.51, 2.52, 2.52, 2.52, 2.52, 2.51, 2.51, 2.51, 2.51, 2.51, 2.5, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.51, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.53, 2.53, 2.53, 2.52, 2.52, 2.52, 2.52, 2.52, 2.51, 2.51, 2.52, 2.52, 2.52, 2.51, 2.51, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.51, 2.51, 2.5, 2.5, 2.5, 2.48, 2.48, 2.48, 2.49, 2.49, 2.49, 2.49, 2.49, 2.48, 2.48, 2.48, 2.48, 2.47, 2.47, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.42, 2.44, 2.44, 2.44, 2.44, 2.43, 2.43, 2.42, 2.42, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.44, 2.43, 2.43, 2.43, 2.43, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.45, 2.44, 2.45, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.43, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.44, 2.44, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.42, 2.42, 2.42, 2.42, 2.42, 2.43, 2.43, 2.43, 2.43, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.41, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.43, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.45, 2.45, 2.45, 2.45, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.48, 2.48, 2.48, 2.49, 2.48, 2.48, 2.48, 2.48, 2.49, 2.48, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.47, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.44, 2.44, 2.44, 2.45, 2.46, 2.46, 2.46, 2.46, 2.46, 2.46, 2.47, 2.46, 2.46, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.46, 2.45, 2.45, 2.45, 2.44, 2.44, 2.45, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.46, 2.46, 2.46, 2.46, 2.46, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.45, 2.46, 2.46, 2.46, 2.46, 2.46, 2.46, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.45, 2.44, 2.43, 2.43, 2.43, 2.43, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.42, 2.42, 2.42, 2.4, 2.4, 2.4, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.42, 2.42, 2.42, 2.41, 2.4, 2.4, 2.4, 2.4, 2.4, 2.39, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.39, 2.39, 2.39, 2.39, 2.39, 2.39, 2.39, 2.4, 2.4, 2.39, 2.39, 2.4, 2.4, 2.4, 2.4, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.42, 2.42, 2.41, 2.42, 2.42, 2.44, 2.44, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.43, 2.44, 2.44, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.41, 2.41, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.42, 2.43, 2.43, 2.43, 2.42, 2.42, 2.42, 2.42, 2.42, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.4, 2.4, 2.41, 2.41, 2.4, 2.41, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.39, 2.39, 2.4, 2.39, 2.39, 2.38, 2.38, 2.39, 2.39, 2.39, 2.39, 2.39, 2.38, 2.38, 2.38, 2.39, 2.39, 2.39, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.41, 2.41, 2.4, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.4, 2.4, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.42, 2.42, 2.43, 2.43, 2.42, 2.42, 2.42, 2.43, 2.43, 2.42, 2.42, 2.42, 2.43, 2.43, 2.43, 2.43, 2.41, 2.42, 2.41, 2.41, 2.42, 2.41, 2.41, 2.42, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.41, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.38, 2.39, 2.4, 2.39, 2.39, 2.39, 2.39, 2.38, 2.38, 2.38, 2.38, 2.39, 2.39, 2.39, 2.39, 2.39, 2.38, 2.38, 2.38, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.36, 2.36, 2.36, 2.35, 2.35, 2.35, 2.35, 2.35, 2.36, 2.35, 2.35, 2.36, 2.36, 2.36, 2.37, 2.37, 2.37, 2.36, 2.36, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.37, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.35, 2.36, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.35, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36, 2.36)\n"
          }
        ]
      }
    },
    {
      "text": "println(occupancyAverageWindows.take(2000).toList.slice(1000, 2000))",
      "user": "anonymous",
      "dateUpdated": "2022-04-09T17:37:36+0200",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://10.200.47.165:4040/jobs/job?id=26",
              "$$hashKey": "object:49580"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1649508654624_1114247885",
      "id": "paragraph_1649508654624_1114247885",
      "dateCreated": "2022-04-09T14:50:54+0200",
      "dateStarted": "2022-04-09T17:37:36+0200",
      "dateFinished": "2022-04-09T17:37:47+0200",
      "status": "FINISHED",
      "$$hashKey": "object:46641",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "List(1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.35, 1.36, 1.36, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.35, 1.35, 1.34, 1.35, 1.35, 1.34, 1.34, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.34, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.31, 1.31, 1.31, 1.31, 1.31, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.31, 1.31, 1.31, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.3, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.31, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.34, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.31, 1.31, 1.31, 1.31, 1.31, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.32, 1.33, 1.33, 1.33, 1.33, 1.33, 1.33, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.34, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35, 1.35)\n"
          }
        ]
      }
    }
  ],
  "name": "pdd2022ZadanieDuze1",
  "id": "2GX5C461P",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/pdd2022ZadanieDuze1"
}
